diff --git a/vllm_ascend/distributed/llmdatadist_c_mgr_connector.py b/vllm_ascend/distributed/llmdatadist_c_mgr_connector.py
index b2023d2a9..d2d004cd5 100644
--- a/vllm_ascend/distributed/llmdatadist_c_mgr_connector.py
+++ b/vllm_ascend/distributed/llmdatadist_c_mgr_connector.py
@@ -293,11 +293,12 @@ class LLMDataDistCMgrConnectorWorker():
 
     def __init__(self, vllm_config: VllmConfig):
         assert vllm_config.kv_transfer_config is not None
+        from vllm import envs
         logger.info("Initialize the LLMDataDistCMgrConnectorWorker")
         # we assume the local node only contains dp and tp, and tp will not communicate inter-node.
         # for any scenario beyond this scope, the functionality of this connector is not guaranteed.
         self.local_rank_on_node = get_world_group().rank % (
-            vllm_config.parallel_config.data_parallel_size_local *
+            envs.VLLM_DP_SIZE_LOCAL *
             vllm_config.parallel_config.tensor_parallel_size)
         self.local_rank = get_world_group().local_rank
         self.local_dp_rank = vllm_config.parallel_config.data_parallel_rank_local
